"""Generate a documentation page summarizing Python vs Rust pipeline timings."""

from __future__ import annotations

import argparse
import datetime as dt
import importlib
import platform
import sys
from pathlib import Path
from typing import Iterable, Sequence

# Ensure the project package is importable when the script runs from a clone.
REPO_ROOT = Path(__file__).resolve().parents[1]
SRC_PATH = REPO_ROOT / "src"


def _load_benchmark_modules() -> tuple[int, object, object, object, dict[str, str]]:
    """Import benchmark helpers after preparing the sys.path."""
    if str(REPO_ROOT) not in sys.path:
        sys.path.insert(0, str(REPO_ROOT))
    if str(SRC_PATH) not in sys.path:
        sys.path.insert(0, str(SRC_PATH))

    pipeline_module = importlib.import_module("benchmarks.pipeline_benchmark")
    constants_module = importlib.import_module("benchmarks.constants")

    return (
        pipeline_module.DEFAULT_ITERATIONS,
        pipeline_module.BenchmarkResult,
        pipeline_module.SCENARIOS,
        pipeline_module.collect_benchmark_results,
        constants_module.SCENARIO_DESCRIPTIONS,
    )


def _ensure_rust_extension_importable() -> None:
    """Attempt to expose a locally built Rust extension for test runs."""
    if importlib.util.find_spec("glitchlings._zoo_rust") is not None:
        return

    build_root = REPO_ROOT / "build"
    if not build_root.exists():
        return

    artifacts = sorted(
        build_root.glob("lib.*/glitchlings/_zoo_rust.*"),
        key=lambda candidate: candidate.stat().st_mtime,
        reverse=True,
    )

    if not artifacts:
        return

    importlib.import_module("glitchlings")

    for artifact in artifacts:
        spec = importlib.util.spec_from_file_location("glitchlings._zoo_rust", artifact)
        if spec is None or spec.loader is None:
            continue
        module = importlib.util.module_from_spec(spec)
        sys.modules["glitchlings._zoo_rust"] = module
        spec.loader.exec_module(module)
        package = sys.modules.get("glitchlings")
        if package is not None and hasattr(package, "__path__"):
            package.__path__.append(str(artifact.parent))
        return


_ensure_rust_extension_importable()

(
    DEFAULT_ITERATIONS,
    BenchmarkResult,
    SCENARIOS,
    collect_benchmark_results,
    SCENARIO_DESCRIPTIONS,
) = _load_benchmark_modules()

DEFAULT_OUTPUT = REPO_ROOT / "docs" / "performance-comparison.md"


def _format_stats(result: BenchmarkResult) -> tuple[str, str, str]:
    python_cell = f"{result.python.mean_ms:.3f} +/- {result.python.stdev_ms:.3f}"
    if result.rust is None:
        return python_cell, "N/A", "N/A"
    rust_cell = f"{result.rust.mean_ms:.3f} +/- {result.rust.stdev_ms:.3f}"
    speedup = "N/A"
    if result.rust.mean_seconds > 0:
        ratio = result.python.mean_seconds / result.rust.mean_seconds
        speedup = f"{ratio:.2f}x"
    return python_cell, rust_cell, speedup


def build_report_content(
    *,
    iterations: int = DEFAULT_ITERATIONS,
    texts: Iterable[tuple[str, str]] | None = None,
    scenarios: Sequence[str] | None = None,
) -> str:
    """Render the Markdown performance report."""
    scenario_names = list(scenarios) if scenarios else list(SCENARIOS.keys())
    invalid = [name for name in scenario_names if name not in SCENARIOS]
    if invalid:
        raise ValueError(f"Unknown scenario(s): {', '.join(invalid)}")

    scenario_results: dict[str, list[BenchmarkResult]] = {}
    rust_available = False
    for name in scenario_names:
        descriptors = SCENARIOS[name]()
        results = collect_benchmark_results(texts, iterations, descriptors)
        scenario_results[name] = results
        rust_available = rust_available or any(result.rust is not None for result in results)

    generated_at = dt.datetime.now(dt.timezone.utc).isoformat()

    lines: list[str] = [
        "# Glitchlings Performance Benchmarks\n\n",
        (
            "This page is generated by `docs/build_performance_report.py` "
            "and refreshed as part of the build pipeline. "
            "It compares the pure-Python orchestrator with the optional Rust acceleration "
            "layer exposed via `glitchlings._zoo_rust` across several representative "
            "pipeline configurations.\n\n"
        ),
        "## Benchmark setup\n\n",
        f"- Samples per measurement: {iterations}\n",
        f"- Python: {platform.python_version()} ({platform.python_implementation()})\n",
        f"- Platform: {platform.system()} {platform.release()} [{platform.machine()}]\n",
        f"- Rust extension built: {'yes' if rust_available else 'no (skipped)'}\n",
        f"- Generated: {generated_at}\n\n",
        "## Results\n\n",
    ]

    for scenario in scenario_names:
        display_name = scenario.replace("_", " ").title()
        lines.append(f"### Scenario: {display_name}\n\n")
        description = SCENARIO_DESCRIPTIONS.get(scenario, "")
        if description:
            lines.append(f"{description}\n\n")
        lines.extend(
            [
                "| Text size | Characters | Python (ms) | Rust (ms) | Speedup |\n",
                "| --- | ---: | ---: | ---: | ---: |\n",
            ]
        )
        for result in scenario_results[scenario]:
            python_cell, rust_cell, speedup_cell = _format_stats(result)
            row = (
                f"| {result.label} | {result.char_count} | {python_cell} | "
                f"{rust_cell} | {speedup_cell} |\n"
            )
            lines.append(row)
        lines.append("\n")

    lines.extend(
        [
            "\n",
            "### Notes\n\n",
            "- Timings use `time.perf_counter` and report population standard deviation.\n",
            (
                "- Rust results appear only when `glitchlings._zoo_rust` imports successfully; "
                "otherwise the column is marked as unavailable.\n"
            ),
            (
                "- Re-run this script after changing pipeline code or the Rust extension "
                "to keep the comparison current.\n"
            ),
        ]
    )

    return "".join(lines)


def parse_args(argv: Sequence[str] | None = None) -> argparse.Namespace:
    """Parse CLI arguments for the report builder."""
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--iterations",
        type=int,
        default=DEFAULT_ITERATIONS,
        help=(
            "Timing samples to collect for each text size "
            f"(default: {DEFAULT_ITERATIONS})"
        ),
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=DEFAULT_OUTPUT,
        help=(
            "Destination Markdown file. Defaults to docs/performance-comparison.md "
            "inside the repository."
        ),
    )
    parser.add_argument(
        "--scenario",
        action="append",
        dest="scenarios",
        choices=sorted(SCENARIOS.keys()),
        help=(
            "Scenario(s) to include in the report (defaults to all). "
            "Can be specified multiple times."
        ),
    )
    return parser.parse_args(argv)


def main(argv: Sequence[str] | None = None) -> None:
    args = parse_args(argv)
    output: Path = args.output
    if not output.is_absolute():
        output = REPO_ROOT / output
    output.parent.mkdir(parents=True, exist_ok=True)

    content = build_report_content(
        iterations=args.iterations, scenarios=args.scenarios
    )
    output.write_text(content, encoding="utf-8")
    relative = output.relative_to(REPO_ROOT)
    print(f"Wrote {relative}")


if __name__ == "__main__":
    main()
