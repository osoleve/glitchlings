{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glitchlings Metrics Framework Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the metrics framework to analyze text transformation effects across different tokenizers.\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install glitchlings[metrics,metrics-tokenizers,metrics-viz]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Computing Metrics for a Single Transformation\n",
    "\n",
    "Let's start with the basics: computing metrics for one text transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glitchlings.metrics.metrics import create_default_registry\n",
    "\n",
    "# Create a registry with all 14 metrics\n",
    "registry = create_default_registry()\n",
    "\n",
    "print(f\"Loaded {len(registry.specs)} metrics:\")\n",
    "for metric_id in sorted(registry.specs.keys()):\n",
    "    spec = registry.specs[metric_id]\n",
    "    print(f\"  - {metric_id:15s}: {spec.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple transformation (swap adjacent tokens)\n",
    "before = [1, 2, 3, 4, 5]  # Token IDs\n",
    "after = [1, 3, 2, 4, 5]   # Tokens 2 and 3 swapped\n",
    "\n",
    "# Compute all metrics\n",
    "results = registry.compute_all(before, after, context={})\n",
    "\n",
    "# Display results\n",
    "print(\"Metric Results:\")\n",
    "print(\"-\" * 50)\n",
    "for key, value in sorted(results.items()):\n",
    "    print(f\"{key:20s}: {value:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "- **ned.value** (Normalized Edit Distance): How different are the sequences?\n",
    "- **lcsr.value** (LCS Retention): What fraction of tokens remain in order?\n",
    "- **rord.value** (Reordering): How much reordering occurred?\n",
    "- **pmr.value** (Position Match Rate): How many tokens stayed at their original positions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Real Text with Tokenizers\n",
    "\n",
    "Now let's analyze how a real glitchling affects tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glitchlings.metrics.core.tokenizers import SimpleTokenizer\n",
    "\n",
    "# Define a glitchling\n",
    "def typogre(text: str) -> str:\n",
    "    \"\"\"Swap 'th' with 'ht'.\"\"\"\n",
    "    return text.replace(\"th\", \"ht\").replace(\"TH\", \"HT\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "# Test text\n",
    "text_before = \"The quick brown fox jumps over the lazy dog.\"\n",
    "text_after = typogre(text_before)\n",
    "\n",
    "print(f\"Before: {text_before}\")\n",
    "print(f\"After:  {text_after}\")\n",
    "print()\n",
    "\n",
    "# Tokenize\n",
    "tokens_before = tokenizer.encode(text_before)\n",
    "tokens_after = tokenizer.encode(text_after)\n",
    "\n",
    "print(f\"Tokens before ({len(tokens_before)}): {tokens_before}\")\n",
    "print(f\"Tokens after ({len(tokens_after)}):  {tokens_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "results = registry.compute_all(tokens_before, tokens_after, context={})\n",
    "\n",
    "# Display key metrics\n",
    "print(\"Key Metrics for Typogre Transformation:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Edit Distance (NED):  {results['ned.value']:.3f}\")\n",
    "print(f\"LCS Retention:        {results['lcsr.value']:.3f}\")\n",
    "print(f\"Reordering Score:     {results['rord.value']:.3f}\")\n",
    "print(f\"Length Ratio:         {results['lr.value']:.3f}\")\n",
    "print(f\"JS Divergence:        {results['jsdiv.value']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Multiple Tokenizers\n",
    "\n",
    "Different tokenizers may respond differently to the same transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from glitchlings.metrics.core.tokenizers import create_huggingface_adapter\n",
    "    \n",
    "    # Create multiple tokenizers\n",
    "    tokenizers = [\n",
    "        SimpleTokenizer(),\n",
    "        create_huggingface_adapter(\"gpt2\"),\n",
    "        create_huggingface_adapter(\"bert-base-uncased\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"Loaded tokenizers:\")\n",
    "    for tok in tokenizers:\n",
    "        print(f\"  - {tok.name}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"HuggingFace tokenizers not available.\")\n",
    "    print(\"Install with: pip install glitchlings[metrics-tokenizers]\")\n",
    "    tokenizers = [SimpleTokenizer()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compare tokenizers\n",
    "text = \"The theory of computation is fundamental.\"\n",
    "\n",
    "comparison_results = []\n",
    "for tokenizer in tokenizers:\n",
    "    tokens_before = tokenizer.encode(text)\n",
    "    tokens_after = tokenizer.encode(typogre(text))\n",
    "    \n",
    "    results = registry.compute_all(tokens_before, tokens_after, context={})\n",
    "    \n",
    "    comparison_results.append({\n",
    "        \"Tokenizer\": tokenizer.name,\n",
    "        \"Tokens Before\": len(tokens_before),\n",
    "        \"Tokens After\": len(tokens_after),\n",
    "        \"NED\": results[\"ned.value\"],\n",
    "        \"LCSR\": results[\"lcsr.value\"],\n",
    "        \"RORD\": results[\"rord.value\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nTokenizer Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Different tokenizers may show different sensitivity to the same transformation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Batch Processing\n",
    "\n",
    "For serious analysis, use batch processing to handle many texts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glitchlings.metrics.core.batch import process_and_write\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Sample texts\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning enables computers to learn from data.\",\n",
    "    \"The theory of computation studies algorithmic complexity.\",\n",
    "    \"Natural language processing analyzes human language.\",\n",
    "    \"Deep neural networks can model complex patterns.\",\n",
    "]\n",
    "\n",
    "# Create temporary output directory\n",
    "output_dir = Path(tempfile.mkdtemp()) / \"metrics_results\"\n",
    "output_dir.mkdir()\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Process batch\n",
    "manifest = process_and_write(\n",
    "    texts=texts,\n",
    "    glitchling_fn=typogre,\n",
    "    glitchling_id=\"typogre\",\n",
    "    registry=registry,\n",
    "    tokenizers=tokenizers,\n",
    "    output_dir=output_dir,\n",
    "    partition_by=[\"tokenizer_id\"],  # One file per tokenizer\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Processed {manifest.num_observations} observations\")\n",
    "print(f\"  Run ID: {manifest.run_id}\")\n",
    "print(f\"  Tokenizers: {', '.join([t.split(':')[0] for t in manifest.tokenizers])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated files\n",
    "parquet_files = list(output_dir.rglob(\"*.parquet\"))\n",
    "print(f\"Generated {len(parquet_files)} Parquet files:\")\n",
    "for pf in parquet_files:\n",
    "    print(f\"  - {pf.relative_to(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Loading and Analyzing Results\n",
    "\n",
    "Load observations from Parquet files for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glitchlings.metrics.viz import load_observations_from_parquet\n",
    "from glitchlings.metrics.viz.aggregate import aggregate_observations\n",
    "\n",
    "# Load all observations\n",
    "all_observations = []\n",
    "for pf in parquet_files:\n",
    "    obs = load_observations_from_parquet(pf)\n",
    "    all_observations.extend(obs)\n",
    "\n",
    "print(f\"Loaded {len(all_observations)} observations\")\n",
    "print(f\"\\nFirst observation:\")\n",
    "obs = all_observations[0]\n",
    "print(f\"  Glitchling: {obs.glitchling_id}\")\n",
    "print(f\"  Tokenizer: {obs.tokenizer_id}\")\n",
    "print(f\"  Tokens: {obs.m} -> {obs.n}\")\n",
    "print(f\"  Metrics: {list(obs.metrics.keys())[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by tokenizer\n",
    "agg_results = aggregate_observations(\n",
    "    all_observations,\n",
    "    group_by=[\"tokenizer_id\"],\n",
    "    metrics=[\"ned.value\", \"lcsr.value\", \"jsdiv.value\"],\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "summary_data = []\n",
    "for result in agg_results:\n",
    "    summary_data.append({\n",
    "        \"Tokenizer\": result[\"tokenizer_id\"],\n",
    "        \"NED (mean)\": result[\"metric_ned.value\"][\"mean\"],\n",
    "        \"NED (std)\": result[\"metric_ned.value\"][\"std\"],\n",
    "        \"LCSR (mean)\": result[\"metric_lcsr.value\"][\"mean\"],\n",
    "        \"LCSR (std)\": result[\"metric_lcsr.value\"][\"std\"],\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary Statistics by Tokenizer:\")\n",
    "print(df_summary.to_string(index=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualizations\n",
    "\n",
    "Now let's create visualizations to understand the patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Radar Chart (Transformation Fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from glitchlings.metrics.viz import create_radar_chart\n",
    "    \n",
    "    # Aggregate all metrics for the glitchling\n",
    "    agg_all = aggregate_observations(\n",
    "        all_observations,\n",
    "        group_by=[\"glitchling_id\"],\n",
    "    )\n",
    "    \n",
    "    # Extract mean values\n",
    "    metrics = {\n",
    "        k.replace(\"metric_\", \"\"): v[\"mean\"]\n",
    "        for k, v in agg_all[0].items()\n",
    "        if k.startswith(\"metric_\") and \".value\" in k\n",
    "    }\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig = create_radar_chart(\n",
    "        metrics,\n",
    "        title=\"Typogre Transformation Fingerprint\",\n",
    "        backend=\"plotly\",  # Interactive\n",
    "        normalization=\"percentile\",\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Visualization libraries not available: {e}\")\n",
    "    print(\"Install with: pip install glitchlings[metrics-viz]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Heatmap (Metric Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from glitchlings.metrics.viz import create_heatmap\n",
    "    \n",
    "    fig = create_heatmap(\n",
    "        all_observations,\n",
    "        metric=\"ned.value\",\n",
    "        row_key=\"input_id\",\n",
    "        col_key=\"tokenizer_id\",\n",
    "        title=\"Edit Distance by Input × Tokenizer\",\n",
    "        backend=\"plotly\",\n",
    "        aggregation=\"median\",\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Heatmap not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Metric Space Embedding (UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from glitchlings.metrics.viz import create_embedding_plot\n",
    "    \n",
    "    if len(all_observations) >= 3:  # Need at least 3 for UMAP\n",
    "        fig = create_embedding_plot(\n",
    "            all_observations,\n",
    "            method=\"umap\",\n",
    "            color_by=\"tokenizer_id\",\n",
    "            title=\"Metric Space (UMAP)\",\n",
    "            backend=\"plotly\",\n",
    "            n_neighbors=min(5, len(all_observations) - 1),\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"Need at least 3 observations for UMAP\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"UMAP not available: {e}\")\n",
    "    print(\"Install with: pip install umap-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Config-Driven Rendering\n",
    "\n",
    "For reproducible research, define visualizations in configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glitchlings.metrics.viz import FigureConfig, render_figure\n",
    "\n",
    "# Define figure configuration\n",
    "config = FigureConfig(\n",
    "    figure_type=\"heatmap\",\n",
    "    title=\"Edit Distance Heatmap\",\n",
    "    params={\n",
    "        \"metric\": \"ned.value\",\n",
    "        \"row_key\": \"input_id\",\n",
    "        \"col_key\": \"tokenizer_id\",\n",
    "        \"backend\": \"plotly\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Render from config\n",
    "try:\n",
    "    fig = render_figure(config, all_observations)\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not render: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. ✅ How to compute metrics for transformations\n",
    "2. ✅ How to compare multiple tokenizers\n",
    "3. ✅ How to use batch processing for scale\n",
    "4. ✅ How to load and aggregate results\n",
    "5. ✅ How to create various visualizations\n",
    "6. ✅ How to use config-driven rendering\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore more glitchlings from the zoo\n",
    "- Try different tokenizers (tiktoken, custom tokenizers)\n",
    "- Define custom metrics for your use case\n",
    "- Create metric lenses to focus on specific aspects\n",
    "- Use sparklines to analyze length sensitivity\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- **README**: `docs/metrics-framework-README.md`\n",
    "- **Planning Doc**: `docs/metrics-framework-plan.md`\n",
    "- **Acceptance Tests**: `docs/metrics-acceptance-tests.md`\n",
    "- **Example Script**: `examples/metrics_complete_example.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
